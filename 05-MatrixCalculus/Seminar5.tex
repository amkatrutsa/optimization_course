\documentclass[12pt]{beamer}
\usepackage{../latex-sty/mypres}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[T2A]{fontenc}


\expandafter\def\expandafter\insertshorttitle\expandafter{%
  \insertshorttitle\hfill%
  \insertframenumber\,/\,\inserttotalframenumber}
\title[Семинар 5]{Методы оптимизации. \\
 Семинар 5. Векторное дифференцирование}
\author{Александр Катруца}
\institute{Московский физико-технический институт\\
Факультет Управления и Прикладной Математики} 
\date{\today}

\begin{document}
\begin{frame}
\maketitle
\end{frame}

\begin{frame}{Напоминание}
\begin{itemize}
\item Сопряжённые множества
\item Свойства сопряжённых множеств
\item Лемма Фаркаша
\end{itemize}
\end{frame}

\begin{frame}{Основные определения}
Более подробно смотрите \href{https://en.wikipedia.org/wiki/Matrix_calculus}{\color{blue}{\texttt{здесь}}}.
Пусть $f: D \rightarrow E$, производная $\frac{\partial f}{\partial x} \in G$:
\begin{table}[!h]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$D$ & $E$ & $G$ & Название\\
\hline
$\bbR$ & $\bbR$ & $\bbR$ & Производная, $f'(x)$\\
\hline
%$\bbR$ & $\bbR^n$ & $\bbR^n$ & $\frac{\partial f_i}{\partial x}$\\
%\hline
%$\bbR$ & $\bbR^{m \times n}$ & $\bbR^{m \times n}$ & $\frac{\partial f_{ij}}{\partial x}$\\
%\hline
$\bbR^n$ & $\bbR$ & $\bbR^n$ & Градиент, $\frac{\partial f}{\partial x_i}$\\
\hline
$\bbR^n$ & $\bbR^m$ & $\bbR^{n \times m}$ & Матрица Якоби, $\frac{\partial f_i}{\partial x_j}$ \\
\hline
$\bbR^{m \times n}$ & $\bbR$ & $\bbR^{m \times n}$ & $\frac{\partial f}{\partial x_{ij}}$\\
\hline
\end{tabular}
\end{table}

Также квадратная $n \times n$ матрица вторых производных $\bH = [h_{ij}]$ в случае $f: \bbR^n \rightarrow \bbR$ называется гессиан и равна $h_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}$.
\end{frame}

\begin{frame}{Основная техника}
\tikzset{
%Define style for boxes
punkt/.style={
rectangle,
draw=black,
minimum height=2em,
text centered}
}
\begin{tikzpicture}
\node[punkt,text width=4cm] (mat_repres) {Матричное представление функции: $f(\bx) = \bx^{\T}\bA\bx, \; \bA \in \bS^n$};
\node[punkt,text width=4cm,below=1cm of mat_repres] (scalar_repres) {Скалярное представление: $f = \sum\limits_{ij}a_{ij}x_ix_j$};
\draw [->,line width=0.1em,>=stealth] (mat_repres) -- (scalar_repres);
\node[punkt,text width=5cm,right=2cm of scalar_repres] (scalar_repres_grad) {Скалярное представление градиента: $\nabla f_k = 2\sum\limits_{j}a_{kj}x_j$};
\draw [->,line width=0.1em,>=stealth] (scalar_repres) -- node [text width=2cm,midway,above,text centered] {$\nabla f_k = \frac{\partial f}{\partial x_k}$} (scalar_repres_grad);
\node[punkt, text width=3cm, above=1cm of scalar_repres_grad] (mat_repres_grad)  {Матричное представление градиента: $\nabla f = 2\bA\bx$};
\draw [->,line width=0.1em,>=stealth] (scalar_repres_grad) -- (mat_repres_grad);
\end{tikzpicture}

\end{frame}

\begin{frame}{Примеры}
\begin{enumerate}
\item Линейная функция: $f(\bx) = \bc^{\T}\bx$
\item Квадратичная форма: $f(\bx) = \frac{1}{2}\bx^{\T}\bA\bx + \mathbf{b}^{\T}\bx$
\item Квадрат $\ell_2$ нормы разности: $f(\bx) = \|\bA\bx - \mathbf{b}\|^2_2$
\item Детерминант: $f(\bX) = \det{\bX}$
\item След: $f(\bX) = \Tr(\bA\bX\bfB)$
\item $f(\bx) = (\bx - \bA\bs)^{\T}\bW(\bx - \bA\bs)$
\item $f(\bA) = (\bx - \bA\bs)^{\T}\bW(\bx - \bA\bs)$
\item $f(\bs) = (\bx - \bA\bs)^{\T}\bW(\bx - \bA\bs)$
\end{enumerate}
\end{frame}

\begin{frame}{Сложная функция}

Пусть $f(\bx) = g(u(\bx))$, тогда $\nabla f(\bx) = \frac{\partial g}{\partial u} \frac{\partial u}{\partial \bx}$

Важно смотреть на размерности и понимать как записывать $\frac{\partial g}{\partial u}$.

Примеры:
\begin{enumerate}
\item $\ell_2$ норма вектора: $f(\bx) = \|\bx\|_2$
%\item Билинейная форма: $f(\bx) = u^{\T}(\bx)\bR v(\bx)$, $\bR \in \bbR^{m \times n}$
\item Экспонента: $f(\bx) = - e^{-\bx^{\T}\bx}$
\end{enumerate}
\end{frame}

\begin{frame}{Резюме}
\begin{itemize}
\item Производная по скаляру
\item Производная по вектору
\item Производная по матрице
\item Производная сложной функции
\end{itemize}
\end{frame}


\end{document}